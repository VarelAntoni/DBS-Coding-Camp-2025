# -*- coding: utf-8 -*-
"""Proyek Pertama_Machine Learning Terapan

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Pr7fu3LMg86osIcs8qVW8Q1jPTOC4u4q
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os

"""import library yang digunakan"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("sahideseker/wine-quality-classification")

print("Path to dataset files:", path)

"""download dataset yang digunakan, disini saya menggunakan dataset Wine Quality Classification yang bertujuan untuk mengelompokkan grade dari wine"""

print(os.listdir(path))

"""print list dari path yang telah di download"""

df = pd.read_csv(os.path.join(path, 'wine_quality_classification.csv'))
df.head()

"""menginisiasi dataset dan menampilkan dataset"""

df.info()

"""melihat informasi dari dataset yang digunakan, terlihat terdapat 1000 row, 5 kolom. memiliki 4 kolom yang bertipe float dan 1 kolom yang bertipe object"""

df.describe(include="all")

"""terlihat bahwa nilai maximum terbilang normal"""

df.isnull().sum()

"""melihat data yang null namun tidak terdapat data yang null"""

df.duplicated().sum()

"""melihat data yang memiliki duplikasi, dan tidak ada data yang duplikat."""

df['quality_label'].value_counts()

"""melihat jumlah value yang dimiliki target, dan terdapat 3 value yaitu medium, high, dan low"""

numerical_cols = df.select_dtypes(include=np.number).columns

num_plots = len(numerical_cols)
num_rows = 5
num_cols = (num_plots + num_rows - 1) // num_rows

fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 15))

if num_cols == 1:
    axes = axes.reshape(num_rows, num_cols)

for i, col in enumerate(numerical_cols):
    row = i // num_cols
    col_idx = i % num_cols
    sns.boxplot(x=df[col], ax=axes[row, col_idx])
    axes[row, col_idx].set_title(col)

for i in range(num_plots, num_rows * num_cols):
    row = i // num_cols
    col_idx = i % num_cols
    fig.delaxes(axes[row, col_idx])

plt.tight_layout()
plt.show()

"""melihat outlier yang terdapat di dataset, namun boxplot terlihat normal"""

categorical_columns = df.select_dtypes(include='object').columns

num_plots = len(categorical_columns)
num_cols = 5
num_rows = (num_plots + num_cols - 1) // num_cols

fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(20, 5 * num_rows))
axes = axes.flatten()

for i, column in enumerate(categorical_columns):
    sns.countplot(x=df[column], ax=axes[i])
    axes[i].set_title(f'Distribusi {column}')
    axes[i].set_xlabel(column)
    axes[i].set_ylabel('Frekuensi')
    axes[i].tick_params(axis='x', rotation=45)

for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

"""melihat distribusi persebaran dari target"""

plt.figure(figsize=(10, 6))
sns.heatmap(df.corr(numeric_only=True), annot=True, cmap='coolwarm', fmt='.2f')
plt.title("Heatmap Korelasi Antar Variabel")
plt.show()

"""melihat korelasi antara numerical features"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
df[numerical_cols] = scaler.fit_transform(df[numerical_cols])

print(df.head())

"""melakukan scaling kepada numerical features"""

from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
df['quality_label'] = label_encoder.fit_transform(df['quality_label'])
print(df[['quality_label', 'quality_label']].head())

"""melakukan encoding terhadap target agar bertipe integer"""

plt.figure(figsize=(10, 6))
sns.heatmap(df.corr(numeric_only=True), annot=True, cmap='coolwarm', fmt='.2f')
plt.title("Heatmap Korelasi Antar Variabel")
plt.show()

"""heatmap dari dataset yang sudah diperbarui"""

from sklearn.model_selection import train_test_split

X = df.drop(columns=['quality_label'])
y = df['quality_label']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"Training set shape: X_train={X_train.shape}, y_train={y_train.shape}")
print(f"Test set shape: X_test={X_test.shape}, y_test={y_test.shape}")

"""ngesplit dataset menjadi X dan y, dan melakukan train_test_split"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Initialize and train the Logistic Regression model
log_reg = LogisticRegression(random_state=42, max_iter=200) # Increased max_iter for convergence
log_reg.fit(X_train, y_train)

# Make predictions
y_pred_log_reg = log_reg.predict(X_test)

# Evaluate the model
print("Logistic Regression Results:")
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_log_reg))
print("\nClassification Report:")
print(classification_report(y_test, y_pred_log_reg))
print("\nAccuracy Score:")
print(accuracy_score(y_test, y_pred_log_reg))

"""melakukan prediksi menggunakan logistic regression dan melakukan evaluasi terhadap model"""

from sklearn.svm import SVC

# Initialize and train the Support Vector Machine model
# Using a linear kernel, but other kernels like 'rbf' can be tried
svm_model = SVC(kernel='linear', random_state=42)
svm_model.fit(X_train, y_train)

# Make predictions
y_pred_svm = svm_model.predict(X_test)

# Evaluate the model
print("\nSupport Vector Machine Results:")
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_svm))
print("\nClassification Report:")
print(classification_report(y_test, y_pred_svm))
print("\nAccuracy Score:")
print(accuracy_score(y_test, y_pred_svm))

"""melakukan prediksi menggunakan SVM dan melakukan evaluasi terhadap model"""

import xgboost as xgb

# Initialize and train the XGBoost model
# Using the objective as 'multi:softmax' for multiclass classification
# and num_class for the number of classes.
# The number of classes is the number of unique values in the target variable.
num_classes = y_train.nunique()

xgb_model = xgb.XGBClassifier(objective='multi:softmax', num_class=num_classes, use_label_encoder=False, eval_metric='mlogloss', random_state=42)

# Train the model
xgb_model.fit(X_train, y_train)

# Make predictions
y_pred_xgb = xgb_model.predict(X_test)

# Evaluate the model
print("\nXGBoost Results:")
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_xgb))
print("\nClassification Report:")
print(classification_report(y_test, y_pred_xgb))
print("\nAccuracy Score:")
print(accuracy_score(y_test, y_pred_xgb))

"""melakukan prediksi menggunakan xgboost dan melakukan evaluasi terhadap model"""

print("\nModel Comparison:")
print(f"Logistic Regression Accuracy: {accuracy_score(y_test, y_pred_log_reg)}")
print(f"SVM Accuracy: {accuracy_score(y_test, y_pred_svm)}")
print(f"XGBoost Accuracy: {accuracy_score(y_test, y_pred_xgb)}")

"""membandingkan ketiga model dan dapat dilihat bahwa model memiliki akurasi yang sangat tinggi yaitu 1.0 karena dataset yang digunakan sangat bersih"""

# Calculate training accuracy for each model
y_train_pred_log_reg = log_reg.predict(X_train)
train_accuracy_log_reg = accuracy_score(y_train, y_train_pred_log_reg)

y_train_pred_svm = svm_model.predict(X_train)
train_accuracy_svm = accuracy_score(y_train, y_train_pred_svm)

y_train_pred_xgb = xgb_model.predict(X_train)
train_accuracy_xgb = accuracy_score(y_train, y_train_pred_xgb)

# Get test accuracies from previous evaluation
test_accuracy_log_reg = accuracy_score(y_test, y_pred_log_reg)
test_accuracy_svm = accuracy_score(y_test, y_pred_svm)
test_accuracy_xgb = accuracy_score(y_test, y_pred_xgb)

# Print the comparison
print("\nTraining vs Test Accuracy Comparison:")
print(f"Logistic Regression: Train={train_accuracy_log_reg:.4f}, Test={test_accuracy_log_reg:.4f}")
print(f"SVM:                 Train={train_accuracy_svm:.4f}, Test={test_accuracy_svm:.4f}")
print(f"XGBoost:             Train={train_accuracy_xgb:.4f}, Test={test_accuracy_xgb:.4f}")

# Visualize the comparison
labels = ['Logistic Regression', 'SVM', 'XGBoost']
train_accuracies = [train_accuracy_log_reg, train_accuracy_svm, train_accuracy_xgb]
test_accuracies = [test_accuracy_log_reg, test_accuracy_svm, test_accuracy_xgb]

x = np.arange(len(labels))  # the label locations
width = 0.35  # the width of the bars

fig, ax = plt.subplots(figsize=(10, 6))
rects1 = ax.bar(x - width/2, train_accuracies, width, label='Training Accuracy')
rects2 = ax.bar(x + width/2, test_accuracies, width, label='Test Accuracy')

# Add some text for labels, title and custom x-axis tick labels, etc.
ax.set_ylabel('Accuracy')
ax.set_title('Training vs Test Accuracy Comparison')
ax.set_xticks(x)
ax.set_xticklabels(labels)
ax.legend()

def autolabel(rects):
    """Attach a text label above each bar in *rects*, displaying its height."""
    for rect in rects:
        height = rect.get_height()
        ax.annotate('%.4f' % height,
                    xy=(rect.get_x() + rect.get_width() / 2, height),
                    xytext=(0, 3),  # 3 points vertical offset
                    textcoords="offset points",
                    ha='center', va='bottom')

autolabel(rects1)
autolabel(rects2)

fig.tight_layout()
plt.show()

"""memvisualisasi akurasi model dan membandingkan antara training accuracy dan test accuracy guna mengetahui jika model terindikasi overfitting. namun dari visualisasi dapat terlihat kalau kedua accuracy seimbang dengan accuracy yang tinggi"""

